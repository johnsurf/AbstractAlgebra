\section{Linear Spaces}

A linear space $\L_p$ of dimension $p$ is the totality of points of $\R_n$ into which a fixed point $P$ is carried by the vectors of a $p$-dimensional vector space $L$ of $\R_n$. Suppose there are given $p+1$ points 
$$P_k = (x^1_k, x^2_k,...,x^n_k), ~k = 0, 1, ..., p$$ which do not lie an any linear space of dimension $<p$. Let us consider the vectors $\a_k = \overrightarrow{P_0 P_k}, k = 1, 2, ...,p$. These $p$ vectors are linearly independent.
For if they were linearly dependent, then the vector space spanned by them would have dimension $<p$. By applying these vectors to this vector space to $P_0$, we obtain a linear space which on the one hand has dimension $<p$, and on
the other contains the points $P_0, P_1, ..., P_p$, which contradicts our assumption.\\

All points of a linear space are equivalent to one another because any point of it can be used to obtain the entire linear space. 

\section{Linear Equations}

Let a system of $m$ linear equations in $n$ unknowns $x^1, x^2, ..., x^n$ be given in the following form:

%\begin{array*}
%  x+y+z = 1\\,
%  x+y+z = \frac{5}{2},\\
%  x+y+z = 5\\
%\end{array*}

\begin{equation}
\begin{array}{cccc}
a_{11}x^1  &+ ~a_{12}x^2   & + \hdots  &+  a_{1n}x^n   = b_1 \\
a_{21}x^1  &+ ~ a_{22}x^2   &+ \hdots  &+  a_{2n}x^n   = b_2 \\
 \cdots       &\cdots            &\cdots      &\cdots                \cdots \\
a_{i1}x^1   &+  ~a_{i2}x^2    & + \hdots  &+  a_{in}x^n    = b_i \\
 \cdots       &\cdots            &\cdots      &\cdots                 \cdots\\
a_{m1}x^1 &+  ~a_{m2}x^2 & + \hdots  & +  a_{mn}x^n = b_m \\
\end{array}
\label{LinearSystem}
\end{equation}

which can be written in the following form based on the rules of matrix multiplication

\begin{equation}
\begin{bmatrix}
a_{11} & a_{12}  & \hdots & a_{1n}  \\
a_{21} & a_{22}  & \hdots & a_{2n}  \\
\hdotsfor[2]{4}\\
a_{i1} & a_{i2}  & \hdots & a_{in}  \\
\hdotsfor[2]{4}\\
a_{m1} & a_{m2}  & \hdots & a_{mn}  \\
\end{bmatrix}
\begin{bmatrix}
x^1\\
x^2\\
.\\
x^i\\
.\\
x^n\\
\end{bmatrix}
=
\begin{bmatrix}
b_1\\
b_2\\
.\\
b_i\\
.\\
b_m\\
\end{bmatrix}
\label{MatrixSystem}
\end{equation}

To every column of the matrix of coefficients we can associate a vector
$$\a_k = \{a_{1k}, a_{2k},...,a_{mk}\}, ~k = 1,2,...,n.$$
The original system of equations can be expressed as a single vector equation
\begin{equation}\a_1 x^2 + \a_2 x^2 + ... + \a_n x^n = \b,\label{VectorSystem}\end{equation} where $\b = \{b_1, b_2, ..., b_m\}$. \\

Consider the vector space $\L$ spanned by the vectors $\a_k, k = 1,...,n$ in $\R_m$. If Eq.(\ref{VectorSystem}) is solvable for $x^1, x^2, ..., x^n$, then $\b$ must be a linear combination of the $\a_k$ and therefore belongs to $\L$. Conversely if $\b$ belongs to $\L$ then Eq.(\ref{VectorSystem}) is solvable. \\

Consider the vector space $\L'$ spanned by the vectors $\a_1, \a_1, ..., \a_n, \b$. A vector $\c$ in $\L'$ is of the form 
$$\c = \lambda_1 \a_1 + \lambda_2 \a_2 + ... + \lambda_n \a_n + \lambda \b.$$ If $\b$ belongs to $\L$, then Eq. (\ref{VectorSystem}) is satisfied by certain numbers $x^1, x^2, ..., x^n$ which can be substituted for 
$\b$ and so $\c$ is a linear combination of the $\a_k, k = 1,..., n$ and therefore belongs to $\L$. In this case $\L' \subseteq \L$ and also $\L \subseteq \L'$ and therefore $\L'$ and $\L$ are identical. 

\begin{theorem}
The equation (\ref{VectorSystem}) is solvable for the $x^i$, if and only if, the maximal number of linearly independent vectors among $\a_1, \a_2, ..., \a_n$ is the same as the maximal number of linearly independent 
vectors among $\a_1, \a_2, ..., \a_n, \b$.
\end{theorem}

\begin{definition}
The {\bf rank} of a matrix is equal to the maximal number of independent column vectors in the matrix. \\ 
\end{definition}

Consider the following matrix

\[
\begin{bmatrix}
a_{11} & a_{12}  & \hdots & a_{1n}  & b_1\\
a_{21} & a_{22}  & \hdots & a_{2n}  & b_2\\
\hdotsfor[2]{5}\\
a_{i1} & a_{i2}  & \hdots & a_{in}  & b_i \\
\hdotsfor[2]{5}\\
a_{m1} & a_{m2}  & \hdots & a_{mn}  & b_m\\
\end{bmatrix}
\]

obtained by appending the vector $\{b_1, b_2, ..., b_m\}$ to the final column. This matrix is called the {\bf augmented} matrix of the system of equations (\ref{LinearSystem}).

\begin{theorem}
Eqs. (\ref{LinearSystem}) are solvable for the $x^i$ if and only if the rank of the coefficient matrix of the system (\ref{LinearSystem}) is equal to the rank of the augmented matrix
\end{theorem}

The following system of equations is called a {\bf homogenous} {\elevenit in the $x^i$}
\begin{equation}
\begin{array}{cccc}
a_{11}x^1  &+ ~a_{12}x^2   & + \hdots  &+  a_{1n}x^n   = 0 \\
a_{21}x^1  &+ ~ a_{22}x^2   &+ \hdots  &+  a_{2n}x^n   = 0 \\
 \cdots       &\cdots            &\cdots      &\cdots                \cdots \\
a_{i1}x^1   &+  ~a_{i2}x^2    & + \hdots  &+  a_{in}x^n    = 0 \\
 \cdots       &\cdots            &\cdots      &\cdots                 \cdots\\
a_{m1}x^1 &+  ~a_{m2}x^2 & + \hdots  & +  a_{mn}x^n = 0 \\
\end{array}
\label{HomogenousSystem}
\end{equation}

Any set of numbers $x^1, x^2, ..., x^n$ which satisfy Eq. (\ref{HomogenousSystem}) are now taken to be the components of an $n$-dimensional vector $\mathfrak{r} = \{x^1, x^2, ..., x^n\}$. 
We call such a vector a {\elevenit vector solution} of the system (\ref{HomogenousSystem}). The totality of vector solutions of the system of equations (\ref{HomogenousSystem}) forms a
vector space.

\begin{theorem}
If the coefficient matrix of the system of equations (\ref{HomogenousSystem}) has rank $r$, then the set of vector solutions of this system is an $(n-r)$-dimensional vector space. 
\end{theorem}

\begin{theorem}
The system of equations (\ref{HomogenousSystem}) has a non-trivial solution, i.e. a solution $x^1, x^2, ..., x^n$ such that not all the $x^i$ vanish, if and only if, the rank of the matrix of 
(\ref{HomogenousSystem}) is less than $n$.
\end{theorem}

\begin{theorem}
If the number of equations in the system of homogenous equations (\ref{HomogenousSystem}) is less than the number of unknowns, then the system must have non-trivial solutions. 
\end{theorem}

\begin{theorem}
All the solutions of a non-homogenous system of equations (\ref{LinearSystem}) are of the form $z = \mathfrak{r} + \eta$, where $\mathfrak{r}$ is a fixed solution of the non-homogenous system (\ref{LinearSystem})
and $\eta$ runs though all solutions of the corresponding homogenous system (\ref{HomogenousSystem}). 
\end{theorem}

Let $s$ be the maximal number of linearly independent rows of the matrix row vectors. Let us assume that the rows of the homogenous system are ordered so that the first $s$ rows are independent to 
start with. This involves no loss of generality since it does not affect that rank. The $i$-th equation of this system is, by definition of the scalar product of two vectors, equivalent to the vector equation
$$ \a_i \cdot \mathfrak{r} = 0,$$ where we set $\mathfrak{r} = \{x^1, x^2, ..., x^n\}$.

Every system of $n$ numbers $x^1, x^2, ..., x^n$ satisfying the first $s$ equation of (the re-ordered) (\ref{HomogenousSystem}), satisfies all of the equations of (\ref{HomogenousSystem}). Because
the first $s$-rows are linearly independent, all the rows satisfy a relation of the form
$$\a_k = \lambda^k_1 \a_1 + \lambda^k_2 \a_2 + ...  + \lambda^k_s \a_s, \hbox{~for~} 1\le k \le m.$$ If we apply the distributive law on the right, we obtain
$$\a_k \cdot \mathfrak{r} =   \lambda^k_1 (\a_1\cdot \mathfrak{r}) + \lambda^k_2 (\a_2 \cdot \mathfrak{r})+ ...  + \lambda^k_s (\a_s\cdot \mathfrak{r}).$$ Since $\a_i \cdot \mathfrak{r} = 0$ for $i = 1,2, ..., s$ the right hand side 
of this last equation becomes $0$, so that $$\a_k \cdot \mathfrak{r} = 0, \quad k = 1,2, ..., m,$$ and in particular for $k = s+1, s+2, ..., m$ as was to be proved.\\

The vector space of all the solutions of the first $s$ equations of (\ref{HomogenousSystem}) is thus identical with the space of solutions of the entire system (\ref{HomogenousSystem}). It dimension is this $n-r$
since $r$ is the rank of the matrix of coefficients in (\ref{MatrixSystem}). It thus follows that the rank of the matrix of the first $s$ equations, i.e of the matrix which consists only of the first $s$ rows of (\ref{HomogenousSystem})
is equal to $n - (n-r) = r$. But the column vectors of this matrix are vectors with $s$ components. Thus the maximal number of linearly independent column vectors of this matrix is $\le s$ since by previous theorem
any $s+1$ vectors of $s$-dimensional vector space are linearly dependent. \\

Therefore the maximal number of linearly independent column vectors of a matrix is at most as large as the maximal number of linearly independent row vectors. \\

Now consider the transpose of the matrix of coefficients of (\ref{HomogenousSystem}). The maximal number of independent column vectors of this matrix is $s$, and the maximal number of row vectors is $r$.
Applying the previous result we also obtain $s\le r$. Hence we have $r \le s$ and $s\le r$ and therefore s = r. Thus we have proved

\begin{theorem}
The maximal number of linearly independent column vectors of a matrix is equal to the maximal number of linearly independent row vectors of that matrix. 
\end{theorem}

\begin{theorem}
With any given vector space $L$ of $\R_n$, we can always associate a system of homogenous linear equations in $n$ unknowns such that all the vectors of $L$, and no others, are vector solutions of this system.
\end{theorem}\

Let $\a_1, \a_2, ..., \a_p$ be a basis of $L$, where we set $\a_i = \{\a_{i1}, \a_{i2}, ..., \a_{in}\}$. If in addition we set 
$$\mathfrak{r} = \{ x^1, x^2, ..., x^n\}$$ then the equations 

\begin{equation}\a_i \cdot \mathfrak{r} = 0, \quad i = 1, 2, ..., p,\label{10}\end{equation}

form a system of homogenous linear equations in the unknowns $x^1, x^2, ..., x^n$ whose matrix is

\[
\begin{bmatrix}
a_{11} & a_{12}  & \hdots & a_{1n} \\
a_{21} & a_{22}  & \hdots & a_{2n} \\
\hdotsfor[2]{4}\\
a_{i1} & a_{i2}  & \hdots & a_{in}  \\
\hdotsfor[2]{4}\\
a_{p1} & a_{p2}  & \hdots & a_{pn}\\
\end{bmatrix}
\]

Let $p$ be the dimension of $L$.The rank of this system must be $n-p$. The totality of vectors which are orthogonal to $L$ is a vector space $L'$ of dimension $n-p$. It consists precisely of all the vector solutions of (\ref{10}).\\

We now seek a those vectors which are othogonal to the vector space $L'$ just found, We chose a basis $\b_1, \b_2, ..., \b_{n-p}$ of $L'$.The $\b_i$ as vectors of $L'$ are orthogonal to $L$, and this in particular to 
$\a_1, \a_2, ... \a_p$. Thus, for $i=1, 2, ..., p$, we have

$$\b_1 \cdot \a_i = 0, ~~\b_2 \cdot \a_2 = 0, ~~..., ~~\b_{n-p} \cdot \a_i = 0.$$

Now just as for $L$, the totality of all vectors orthogonal to $L'$ consists of all the vector solutions of the equations
\begin{equation}\b_1 \cdot \mathfrak{r} = 0, ~~\b_2 \cdot \mathfrak{r} = 0, ~~..., ~~\b_{n-p} \cdot \mathfrak{r} = 0.\label{11}\end{equation}
 
 Thus the $\a_i, i =1,2, ..., p$ are orthogonal to $L'$. But the vectors which are orthogonal to $L'$ form a vector space $L''$ of dimension $n - (n-p) = p$.Since the vectors $\a_1, \a_2, ..., \a_p$ are in $L''$ and are 
 linearly independent, then form a basis of $L''$. Therefore $L''$ is identical with $L$. Thus the vectors of $L$ are precisely the vector solutions of equations (\ref{11}), i.e, of a certain system of $n-p$ linear homogenous equations.
 
 \begin{theorem}
 The point solutions of a solvable system of equations of the type (\ref{LinearSystem}) form a linear space. This space has dimensions $n-r$m where $r$ is the rank of the coefficient matrix of the system. Conversely, every linear
 space may be represented as the totality of all point solutions of some suitable system of linear equations. 
 \end{theorem}