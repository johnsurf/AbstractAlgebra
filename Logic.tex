\section{Logic}

\begin{definition}
If a statement $A$ leads logically to another statement $B$ we say that `if $A$, then $B$' and write $A \rightarrow B$. Another way to think about implication is that $A\rightarrow B$ can be thought of as `if $A$ is true, then $B$ must be true' (`$B$, if $A$') or that $B$ is {\twelveit necessary} for $A$. This can also be expressed as `$A$ {\twelveit only if} $B$'.
\end{definition}

$A\rightarrow B$ implies that $A$ being true is sufficient for $B$ to be true. If the we have both $A\rightarrow B$ and $B\rightarrow A$, then we can say that $A$ is {\twelveit necessary and sufficient} for $B$ -- which can also be stated as $A \iff B$.\\

\begin{definition}
$n$-dimensional Affine Space $\R_n$. An ordered $n$-tuple of real numbers, i.e., a system of $(x_1, x_2, ..., x_n)$ of $n$ real numbers in a definite order, is called a point ($n$ a positive integer). The numbers
$x_1,x_2, ...,x_n$ are called the coordinates of the point; in particular $x_1$ is called its first, $x_2$ its second, ..., $x_n$ its $n$-th coordinate. Two points $P=(x_1,x_2,...,x_n)$ and $Q = (y_1,y_2,...,y_n)$ are said to 
be the {\elevenit same} or to coincide if and only if $x_1 = y_1, x_2 = y_2, ..., x_n = y_n$. The totality of all $n$-tuples of real numbers is called $n$-dimensional Affine Space and is denoted by $\R_n$.
\end{definition}

\begin{definition}{\bf The laws of addition}:\\
\begin{itemize}
\item[A1:] The Commutative Law of addition: $x+ y = y + x$ for all pairs of numbers $x$ and $y$.
\item[A2:] The Associative Law of addition: $(x+y) + z = x + (y+z)$ for any three numbers $x$, $y$ and $z$.
\item[A3:] The existence of a zero. There is a number, called the zero and denoted by $0$, which has the property that
$$ x + 0 = x \hbox{~and~} 0 + x = x \hbox{~for all numbers~} x.$$
\item[A4:] The existence of negatives: Corresponding to each number $x$ there is a number called the negative of $x$ and written $-x$ which satisfies
$$x + -x = 0\hbox{~and~} -x + x = 0.$$
\end{itemize}
\end{definition}

\begin{definition}{\bf Distributive Law}
\begin{itemize}
\item[D1:] The Distributive Law:
\begin{eqnarray*}
(x+y)z &=& xz + yz\\
x(y+z) &=& xy + xz
\end{eqnarray*}
\end{itemize}
\end{definition}

%\clearpage
\begin{definition}{\bf The laws of multiplication}:\\
\begin{itemize}
\item[M1:] The Commutative Law of multiplication : $x y = y x$ for all pairs of numbers $x$ and $y$.
\item[M2:] The Associative Law of multiplication: $(xy)z = x (yz)$ for any three numbers $x$, $y$ and $z$.
\item[M3:] The existence of a unity. There is a number, called the unity and denoted by $1$, which has the property that
$$ x1= x \hbox{~and~} 1x = x \hbox{~for all numbers~} x.$$
\item[M4:] The existence of inverses: Corresponding to each number $x$ there is a number called the inverse of $x$ and written $x^{-1}$ which satisfies
$$xx^{-1} = 1\hbox{~and~} x^{-1}x = 1.$$
\end{itemize}
\end{definition}

\section{Mappings}
\begin{definition}
An injective function $f: X\rightarrow Y$ (also known as injection, or one-to-one function) is a function $f$ that maps distinct elements of its domain to distinct elements; that is, $f(x_1) = f(x_2$) implies $x_1 = x_2$. 
Equivalently, $x_1 \ne x_2$ implies $f(x_1) \ne f(x_2)$ in the equivalent contrapositive statement.
\end{definition}

\begin{definition}
A surjective function  $f: X\rightarrow Y$ (also known as surjection, or onto function) is a function $f$ that every element $y$ can be mapped from some element $x$ so that $f(x) = y$. 
In other words, every element of the function's codomain is the image of at least one element of its domain. It is not required that $x$ be unique; the function $f$ 
may map one or more elements of the set $X$ to the same element of the set $Y$.
\end{definition}

\begin{definition}{\bf Homorphisms and Mappings}:\\
Monomorphism: 1-1, Epimorphism: onto, Isomorphism: 1-1 and onto.\\ Homomorphisms preserve structure (i.e. multiplication and addition in Groups and Rings). \\
Homomorphisms that have the same object and image space are called Endomorphisms.\\ And Endomorphism that is also an Isomorphism is called and Automorphism.\\
In a Group G, the mappings of G into itself, given by $g\rightarrow x^{-1} g x$, where $x$ is any fixed element of G, is an Automorphism, known as an Inner Automorphism.\\ 
\end{definition}

Suppose $f: A \rightarrow B$ and $g: B\rightarrow C$. Hence $(g\circ f) : A \rightarrow C$ exists. Show\\
(a) If $f$ and $g$ are 1-1 (monomorphisms), then $g\circ f$ is 1-1:\\
~~~Suppose $(g\circ f)(x) = (g\circ f)(y)$. Then $g(f(x)) = g(f(y))$. Because $g$ is 1-1, $f(x) = f(y)$ and because $f$ is 1-1, $x = y$. Therefore $(g\circ f)(x) = (g\circ f)(y)$, implies $x = y$; hence $g\circ f$ is 1-1. \\
(b) If $f$ and $g$ are onto (epimorphisms), then $g\circ f$ is onto.\\
~~~Suppose $c \in C$. Because $g$ is onto, there exists $b \in B$ for which $g(b) = c$. Because $f$ is onto, there exists $a\in A$ for which $f(a) = b$. Thus  $(g\circ f)(a) = g(f(a)) = g(b) = c$, Hence $g\circ f$ is onto. \\ \\
(c) If $g\circ f$ is 1-1, then $f$ is 1-1.\\ 
~~~Prove contrapositive, Suppose $f$ is  not 1-1., Then there exists distinct elements $x,y \in A$ for which $f(x) = f(y)$. Then $(g\circ f)(x) = g(f(x)) = g(f(y)) = (g\circ f)(y)$. Hence $g\circ f$ is not 1-1.Therefore if $g\circ f$ is 1-1, then $f$ must be 1-1.\\
(d) If $g\circ f$ is onto, then $g$ is onto.\\
~~~Prove contrapositive, If $a \in A$, then $(g\circ f)(a) = g(f(a)) \in g(B)$. Hence $(g\circ f)(A) \subseteq g(B)$. 
Suppose $g$ is not onto. Then $g(B)$ is properly contained in $C$ and so $(g\circ f)(A)$ is properly contained in C; thus $g\circ f$ is not onto. Therefore if $g\circ f$ is onto, then $g$ must be onto.\\

\section{Linear Dependence}
\begin{definition}
Vectors are directed segments. The vector in $\R_n$ whose components are $a_1, a_2,...,a_n$ is denoted by $$\a = \{a_1, a_2, ..., a_n\}\hbox{~(braces)}.$$
The zero vector in $\R_n$ is denoted as $$\o = \{0,0,...,0\},$$ with $n$ components each 0. 
\end{definition}

\begin{definition}We call $p$ vectors $\a_1, \a_2, ..., \a_p$ {\bf linearly dependent} if there are $p$ numbers $\lambda_1, \lambda_2,...,\lambda_p$ not all $0$, such that 
$$\lambda_1 \a_1 + \lambda_2 \a_2 +...+ \lambda_p \a_p = \o\hbox{~ (the zero vector)}.$$
\end{definition}

If no such $p$ numbers exist, then the $p$ vectors $\a_1, \a_2, ...,\a_p$ are called {\bf linearly independent}.

\begin{theorem} 
It a subset of the $p$ vectors $\a_1, \a_2,...,\a_p$ is linearly dependent, then the set of $p$ vectors is itself linearly dependent.\label{Th1_1}
\end{theorem}

Suppose the first $r$ vectors $\a_1, \a_2, ..., \a_r, (r < p) $are linearly dependent. Then there are $r$ numbers $\lambda_1, \lambda_2, ..., \lambda_r$ not all zero such that 
$$\lambda_1 \a_1 + \lambda_2 \a_2 + ... + \lambda_r \a_r = \o.$$ Setting $\lambda_{r+1}, \lambda_{r+2}, ..., \lambda_p$ to zero we have
$$\lambda_1 \a_1 + \lambda_2 \a_2 + ... + \lambda_r \a_r + \lambda_{r+1}\a_{r+1} +... + \lambda_p \a_p = \o.$$ where the numbers $\lambda_1, \lambda_2, ..., \lambda_p$ are not all zero. Therefore the 
$p$ vectors $\a_1, \a_2, ..., \a_p$ are linearly dependent. 

\begin{theorem}
If the $p$ vectors $\a_1, \a_2, ..., \a_p$ are linearly independent, then so is every subset of these $p$ vectors. \label{Th1_2}
\end{theorem}

For otherwise, by Theorem \ref{Th1_1}, the $p$ vectors would be linearly dependent.

\begin{theorem}
If the $p$ vectors $\a_1, \a_2, ..., \a_p$ are linearly dependent and if $p>1$, then at least one of these vectors
is a linear combination of the others.\label{Th1_3}
\end{theorem}

For, there are numbers $\lambda_i$, which do not all vanish such that $$\lambda_1 \a_1 + \lambda_2 \a_2 +...+ \lambda_p \a_p = \o.$$ Suppose $\lambda_p \ne 0$. The we can solve for $\a_p$,
$$\a_p = -{\lambda_1\over \lambda_p}\a_1 -{\lambda_2\over \lambda_p}\a_2 - ... - {\lambda_{p-1}\over \lambda_p} \a_{p-1},$$ i.e. $\a_p$ is a linear combination of the vectors $\a_1, \a_2, ..., \a_{p-1}$.  

\begin{theorem}
If one of the vectors $\a_1, \a_2, ..., \a_p$ is a linear combination of the others, then vectors $\a_1,\a_2,...,\a_p$ are linearly dependent.\label{Th1_4}
\end{theorem}

Suppose $$\a_1 = \lambda_2 \a_2 + \lambda_3 \a_3 +... + \lambda_p \a_p.$$ Then we have that 
$$\a_1 - \lambda_2 \a_2 - \lambda_3 \a_3 -... - \lambda_p \a_p = \o,$$ and since the coefficient of $\a_1$ is not zero, the vectors $\a_1, \a_2, ... , \a_p$ are linearly dependent. 

\begin{theorem}
If the vectors $\a_1, \a_2, ..., \a_p$ are linearly independent, and if the vectors $\a_1, \a_2,...\a_p,\b$ are linearly dependent, then $\b$ is a linear
combination of $\a_1, \a_2, ..., \a_p$.\label{Th1_5}
\end{theorem}

We have a relation of the form $$\lambda_1 \a_1 + \lambda_2 \a_2 +... + \lambda_p \a_p + \lambda_{p+1} \b =\o,$$ where not all the $\lambda_i$ vanish. Now
$\lambda_{p+1}$ cannot be 0 because the last term would drop out and then all the other $\lambda_i$ would vanish because of the linear independence of the $\a_i$ vectors. Therefore
$\lambda_{p+1} \ne 0$ and so we have 
$$\b = -{\lambda_1\over \lambda_{p+1}}\a_1 -{\lambda_2\over \lambda_{p+1}}\a_2 - ... - {\lambda_p\over \lambda_{p+1}} \a_p.$$


\begin{theorem}
If the vectors $\a_1, \a_2, ..., \a_p$ are linearly independent, and if $\b$ is a not a linear combination of $\a_1, \a_2, ..., \a_p$, then the vectors
$\a_1, \a_2,...\a_p,\b$ are linearly independent.\label{Th1_6}
\end{theorem}

Theorem \ref{Th1_6} follows from Theorem \ref{Th1_5}.